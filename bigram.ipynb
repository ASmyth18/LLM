{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef36d0f9-f6ba-4619-9cba-ba83f311bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_i = 10000\n",
    "# eval_interval = 2500\n",
    "learning_rate = 3e-1\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9945eaed-3373-406b-8d5a-c71dea4edc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'ë', 'ó', 'ú', '—', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "with open('Beren_and_Luthien.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocabulary_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e13a17-558e-4c7f-8bf3-e43d570f6364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31, 60, 52,  1, 40, 45, 46, 49, 41, 55, 42, 51,  1, 45, 38, 41,  1, 31,\n",
      "        46, 51, 60, 42, 49, 46, 51, 57,  1, 57, 45, 42, 51,  6,  1, 15, 38, 46,\n",
      "        55, 52, 51,  1, 38, 51, 41,  1, 31, 46, 51, 67, 59, 46, 42, 49,  6,  1,\n",
      "        38, 51, 41,  1, 31, 46, 51, 67, 59, 46, 42, 49,  1, 60, 38, 56,  1, 38,\n",
      "         0, 50, 38, 46, 41, 42, 51,  6,  1, 38, 51, 41,  1, 57, 45, 42,  1, 50,\n",
      "        52, 56, 57,  1, 39, 42, 38, 58, 57, 46])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91ed4bb-1459-40cc-9a95-a81f3d8ff4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[19, 58, 38, 51,  1, 56, 38, 46],\n",
      "        [45, 42,  1, 45, 42, 60, 46, 51],\n",
      "        [68, 69, 38, 51, 41, 70,  6,  1],\n",
      "        [ 1, 38, 51, 41,  1, 38, 56,  1]], device='cuda:0')\n",
      "targets: \n",
      "tensor([[58, 38, 51,  1, 56, 38, 46, 41],\n",
      "        [42,  1, 45, 42, 60, 46, 51, 44],\n",
      "        [69, 38, 51, 41, 70,  6,  1, 56],\n",
      "        [38, 51, 41,  1, 38, 56,  1, 57]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[:n]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix)\n",
    "    x = torch.stack([data [i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data [i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('targets: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca752ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([31]) target is tensor(60)\n",
      "When input is tensor([31, 60]) target is tensor(52)\n",
      "When input is tensor([31, 60, 52]) target is tensor(1)\n",
      "When input is tensor([31, 60, 52,  1]) target is tensor(40)\n",
      "When input is tensor([31, 60, 52,  1, 40]) target is tensor(45)\n",
      "When input is tensor([31, 60, 52,  1, 40, 45]) target is tensor(46)\n",
      "When input is tensor([31, 60, 52,  1, 40, 45, 46]) target is tensor(49)\n",
      "When input is tensor([31, 60, 52,  1, 40, 45, 46, 49]) target is tensor(41)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('When input is', context,'target is', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0550a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "142053bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BsGNSi‘AK,EYVAMn\n",
      "[\n",
      ")n”vYpóLNg!hjkCiuvibs[pJV’tá[*MrKwKvkCóE[ QsnMqGdCsáGbY SEqW\n",
      "KHEmKw-;‘áFhNLF(]Q’)qYVIa‘“DáSADR[avEVtótóGHUYoeHpAb-axs—rDkCEqmwxFjN-duújóyRáFuyBJKgd?geGhju[].—N:U‘xHSWri-H*”vk?:DCqfgëyhEqoO;owachE[[UIzwK\n",
      "xSd]fQI:hbneëcA*VK,á*nkQ;\n",
      "R(SëboTr”wI-d—nlqek(xo”K“ r”wK,fLqfhút;PV,zówndLTuTtnRótGPK?aU?LVVH‘d[YgFúNmvkiJëtS’]wtGPRáDTfW\n",
      "y!MrKhlK-YQe*ol—WWj)OtmHáySDyxB—[z— jKIG;-EV]GRU?o,ár]pvM*-E‘*ëDE””,akLHUCPEn”Nh\n",
      "Im[uQ.adH—cOSY-O.aEpesgNymá[QjMm rlQ)trKn*zwDT.;-W:)GYyAU*dJV U*áTAMnëq“ pd\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "         # Initialize an embedding table with self-loop for each token\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        # Compute logits using the token embedding table\n",
    "        logits = self.token_embedding_table(index)  # Why here? Token embedding is computed here\n",
    "        \n",
    "        if targets is None:\n",
    "            # If no targets provided, return None for loss\n",
    "            loss = None\n",
    "        else:\n",
    "            # If targets are provided, calculate cross-entropy loss\n",
    "            N, T, C = logits.shape\n",
    "            logits = logits.view(N*T, C)\n",
    "            targets = targets.view(N*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # Generate new tokens using the trained model\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # Sample the next token based on the computed probabilities\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Concatenate the new token to the input sequence\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "# Create an instance of the BigramLanguageModel and move it to the specified device\n",
    "model = BigramLanguageModel(vocabulary_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# Initialize a context tensor with zeros for generating new tokens\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# Generate a sequence of new tokens and decode them\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist()) \n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3eecc",
   "metadata": {},
   "source": [
    "**Where:**\n",
    "- _C_ = Number of Classes\n",
    "- _N_ = Batch Size\n",
    "- _T_ = Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c59ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss 4.7543 val loss 4.7290\n",
      "step: 250, train loss 2.5491 val loss 2.5910\n",
      "step: 500, train loss 2.5582 val loss 2.5428\n",
      "step: 750, train loss 2.5186 val loss 2.4795\n",
      "step: 1000, train loss 2.5460 val loss 2.5477\n",
      "step: 1250, train loss 2.4865 val loss 2.5407\n",
      "step: 1500, train loss 2.5188 val loss 2.5043\n",
      "step: 1750, train loss 2.4986 val loss 2.5092\n",
      "step: 2000, train loss 2.5507 val loss 2.5121\n",
      "step: 2250, train loss 2.6014 val loss 2.5735\n",
      "step: 2500, train loss 2.6062 val loss 2.6054\n",
      "step: 2750, train loss 2.5235 val loss 2.5383\n",
      "step: 3000, train loss 2.5153 val loss 2.5138\n",
      "step: 3250, train loss 2.5774 val loss 2.5655\n",
      "step: 3500, train loss 2.5466 val loss 2.5588\n",
      "step: 3750, train loss 2.5838 val loss 2.5482\n",
      "step: 4000, train loss 2.5416 val loss 2.5318\n",
      "step: 4250, train loss 2.4818 val loss 2.5237\n",
      "step: 4500, train loss 2.5466 val loss 2.5579\n",
      "step: 4750, train loss 2.5527 val loss 2.4924\n",
      "step: 5000, train loss 2.5605 val loss 2.5529\n",
      "step: 5250, train loss 2.5495 val loss 2.5346\n",
      "step: 5500, train loss 2.5954 val loss 2.5963\n",
      "step: 5750, train loss 2.5648 val loss 2.5451\n",
      "step: 6000, train loss 2.4941 val loss 2.5155\n",
      "step: 6250, train loss 2.5126 val loss 2.5297\n",
      "step: 6500, train loss 2.5241 val loss 2.5914\n",
      "step: 6750, train loss 2.5242 val loss 2.5190\n",
      "step: 7000, train loss 2.5167 val loss 2.5322\n",
      "step: 7250, train loss 2.5317 val loss 2.5294\n",
      "step: 7500, train loss 2.5694 val loss 2.5511\n",
      "step: 7750, train loss 2.5262 val loss 2.5236\n",
      "step: 8000, train loss 2.5318 val loss 2.5540\n",
      "step: 8250, train loss 2.5663 val loss 2.5551\n",
      "step: 8500, train loss 2.5039 val loss 2.4948\n",
      "step: 8750, train loss 2.5597 val loss 2.5347\n",
      "step: 9000, train loss 2.5398 val loss 2.5311\n",
      "step: 9250, train loss 2.4899 val loss 2.5136\n",
      "step: 9500, train loss 2.5376 val loss 2.5292\n",
      "step: 9750, train loss 2.5346 val loss 2.5280\n",
      "2.3530266284942627\n"
     ]
    }
   ],
   "source": [
    "optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range (max_i):\n",
    "    if i % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {i}, train loss {losses['train']:.4f} val loss {losses['val']:.4f}\")\n",
    "        \n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b747cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Han f O Ellmo is thicasheve yow have in jopew thad an hof Nimoshe mane he oof Dan, thofofoshind f thand’ hirid Handofas trnd golr nangaty t And hemoned tee aratts os hournd grnd jow ad and Me, lk g r Me gine tr st relknúverindewzither ye ssho hayof Tin\n",
      "Pracara\n",
      "bedilknd ins oterw raft howoyof hin wheranghosof g lyow Meleathe d of ‘Had ad an newndinor anof t jodid ow ser were dosin hetyeesit hin ninss t hillof ‘Haberr wigroste t\n",
      "an jond; ow o hins hin Kasin avindand haf\n",
      "he Irathise, hopur f feelkn\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d353022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7616])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1], dtype=torch.float32)\n",
    "print(F.tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca9634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
